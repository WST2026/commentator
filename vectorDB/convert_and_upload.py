import os
import sys
import json
import uuid
import yaml
import argparse
import hashlib
from opensearchpy import OpenSearch, helpers

# ğŸ”§ ê¸°ë³¸ ì„¤ì •
CONFIG_PATH = "../config/upload_config.yaml"
INPUT_JSON = "../data_collection/bing_articles_full.json"
BULK_JSONL = "bulk.jsonl"

# ğŸ”— OpenSearch í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
client = OpenSearch("http://localhost:9200")

# âš™ï¸ ì„¤ì • ë¡œë“œ
with open(CONFIG_PATH, "r", encoding="utf-8") as f:
    config = yaml.safe_load(f)

index_name = config.get("index_name", "default_index")
id_strategy = config.get("id_strategy", "sequential")  # uuid | sequential | hash

# âœ… ì¸ë°ê·¸ìŠ¤ ìƒì„± (ë²¡í„° í•„ë“œ í¬í•¨)
def create_index_if_not_exists(index_name):
    if client.indices.exists(index=index_name):
        print(f"[0] ì¸ë°ê·¸ìŠ¤ '{index_name}' ì´ë¯¸ ì¡´ì¬")
        return

    mapping = {
        "settings": {
            "index": {
                "number_of_shards": 1,
                "number_of_replicas": 0,
                "knn": True
            }
        },
        "mappings": {
            "properties": {
                "id": {"type": "keyword"},
                "title": {
                    "type": "text",
                    "fields": {"keyword": {"type": "keyword"}}
                },
                "content": {
                    "type": "text",
                    "fields": {"keyword": {"type": "keyword"}}
                },
                "url": {"type": "keyword"},
                "datetime": {"type": "text"},
                "project_name": {"type": "keyword"},
                "file_name": {"type": "keyword"},
                "page": {"type": "integer"},
                "embedding": {
                    "type": "knn_vector",
                    "dimension": 384
                }
            }
        }
    }

    client.indices.create(index=index_name, body=mapping)
    print(f"[0] ì¸ë°ê·¸ìŠ¤ '{index_name}' ìƒì„± ì™„ë£Œ")

# âœ… ID ìƒì„± ì „ëµ
def generate_id(item, i):
    if id_strategy == "sequential":
        return str(i + 1)
    elif id_strategy == "hash":
        base = item.get("title", "") + item.get("content", "")
        return hashlib.md5(base.encode("utf-8")).hexdigest()
    else:
        return str(uuid.uuid4())

# âœ… bulk.jsonl ìƒì„± (ì„ë² ë”© í¬í•¨)
def convert_json_to_bulk():
    with open(INPUT_JSON, "r", encoding="utf-8") as f:
        data = json.load(f)

    with open(BULK_JSONL, "w", encoding="utf-8") as out:
        for i, item in enumerate(data):
            doc_id = generate_id(item, i)
            text = item.get("content", "")
            embedding = embedding_model.encode(text).tolist()

            meta = {"index": {"_index": index_name, "_id": doc_id}}
            doc = {
                "title": item.get("title", ""),
                "content": text,
                "url": item.get("url", ""),
                "datetime": item.get("datetime", ""),
                "project_name": "agentic_rag",
                "file_name": os.path.basename(INPUT_JSON),
                "page": 1,
                "id": doc_id,
                "embedding": embedding
            }
            out.write(json.dumps(meta, ensure_ascii=False) + "\n")
            out.write(json.dumps(doc, ensure_ascii=False) + "\n")

    print(f"[1] bulk í¬ë§· ë³€í™” ì™„ë£Œ: {BULK_JSONL} ({len(data)}ê°œ ë¬¸ì„œ)")

# âœ… ì—…ë¡œë“œ
def upload_bulk_jsonl():
    with open(BULK_JSONL, "r", encoding="utf-8") as f:
        lines = f.readlines()

    bulk_lines = [json.loads(line) for line in lines]
    actions = []
    for i in range(0, len(bulk_lines), 2):
        meta = bulk_lines[i]["index"]
        doc = bulk_lines[i + 1]
        action = {
            "_op_type": "index",
            "_index": meta["_index"],
            "_id": meta["_id"],
            "_source": doc
        }
        actions.append(action)

    helpers.bulk(client, actions)
    print(f"[2] ì—…ë¡œë“œ ì™„ë£Œ: {len(actions)}ê°œ ë¬¸ì„œ ì—…ë¡œë“œë¨")
    os.remove(BULK_JSONL)
    print(f"[3] ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {BULK_JSONL}")

# âœ… ë¬¸ì„œ ì‚­ì œ (idì™€ ì¡°ê±´ìœ¼ë¡œ)
def delete_documents(field=None, value=None):
    if not client.indices.exists(index=index_name):
        print(f"âŒ ì¸ë°ê·¸ìŠ¤ '{index_name}' ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
        return

    if not field or not value:
        confirm = input("âš ï¸ ì¸ë°ê·¸ìŠ¤ ì „ì²´ë¥¼ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (yes/no): ").strip().lower()
        if confirm == "yes":
            client.indices.delete(index=index_name)
            print(f"ğŸ—‘ï¸ ì¸ë°ê·¸ìŠ¤ '{index_name}' ì‚­ì œ ì™„ë£Œ")
        else:
            print("âŒ ì‚­ì œ ì¤‘ì§€")
        return

    # id ê°’ìœ¼ë¡œ ë°”ë¡œ ì‚­ì œ
    if field == "id":
        res = client.delete(index=index_name, id=value, ignore=[404])
        if res.get("result") == "deleted":
            print(f"ğŸ—‘ï¸ ID '{value}' ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ")
        else:
            print(f"âŒ ID '{value}' ë¬¸ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ")
    else:
        # match ì¡°ê±´ìœ¼ë¡œ ê²€ìƒ‰ í›„ ì‚­ì œ
        query = {"match": {field: value}}
        search_res = client.search(index=index_name, body={"query": query, "_source": False, "size": 1000})
        hits = search_res["hits"]["hits"]

        if not hits:
            print(f"âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (field: {field}, value: {value})")
            return

        for hit in hits:
            client.delete(index=index_name, id=hit["_id"], ignore=[404])
        print(f"ğŸ—‘ï¸ ê²€ìƒ‰ìœ¼ë¡œ ì°¾ì€ {len(hits)}ê°œ ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ")

# âœ… ì¸ë°ê·¸ìŠ¤ í™•ì¸
def check_index():
    print("ğŸ”ª ì¸ë°ê·¸ìŠ¤ ìƒíƒœ ì ê²€ ì¤‘...")
    if client.indices.exists(index=index_name):
        count = client.count(index=index_name)["count"]
        print(f"âœ… ì¸ë°ê·¸ìŠ¤ '{index_name}' ì¡´ì¬ (ë¬¸ì„œ ìˆ˜: {count})")
    else:
        print(f"âŒ ì¸ë°ê·¸ìŠ¤ '{index_name}' ì¡´ì¬í•˜ì§€ ì•ŠìŒ")

# âœ… ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸° (+ ê²€ìƒ‰)
def preview_documents(size=5, field=None, value=None):
    if not client.indices.exists(index=index_name):
        print(f"âŒ ì¸ë°ê·¸ìŠ¤ '{index_name}' ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
        return

    if field and value:
        if field == "id":
            query = {"term": {field: value}}
        else:
            query = {"match": {field: value}}
    else:
        query = {"match_all": {}}

    res = client.search(index=index_name, body={"size": size, "query": query})
    hits = res["hits"]["hits"]

    if not hits:
        print(f"ğŸ” ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (field: {field}, value: {value})")
        return

    for i, hit in enumerate(hits, 1):
        source = hit["_source"]
        print(f"\nğŸ“„ Document {i} (ID: {source['id']})")
        print(json.dumps({
            "title": source.get("title", ""),
            "content": source.get("content", ""),
            "url": source.get("url", ""),
            "datetime": source.get("datetime", "")
        }, indent=2, ensure_ascii=False))

# âœ… ëŒ€í™”í˜• CLI
def interactive_cli():
    print("\në¬´ì—…ì„ í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
    print("1. ì—…ë¡œë“œ (upload)")
    print("2. ì¸ë°ê·¸ìŠ¤ í™•ì¸ (check)")
    print("3. ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸° (preview)")
    print("4. ë¬¸ì„œ ì‚­ì œ (delete)")
    print("5. ë²¡í„° ê²€ìƒ‰ (search)")

    cmd = input("ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()

    if cmd == "1":
        print("ğŸš€ ì—…ë¡œë“œ ì‹œì‘")
        create_index_if_not_exists(index_name)
        convert_json_to_bulk()
        upload_bulk_jsonl()
    elif cmd == "2":
        check_index()
    elif cmd == "3":
        field = input("ê²€ìƒ‰í•  í•„ë“œ (id, title, content): ").strip()
        value = input("ê²€ìƒ‰ì–´: ").strip()
        size = input("ì¶œë ¥ ê°œìˆ˜ (ê¸°ë³¸ 5): ").strip()
        size = int(size) if size.isdigit() else 5
        preview_documents(size=size, field=field, value=value)
    elif cmd == "4":
        field = input("ì‚­ì œí•  í•„ë“œ (id, title, content): ").strip()
        value = input("ê²€ìƒ‰ì–´: ").strip()
        delete_documents(field=field, value=value)
    elif cmd == "5":
        print("(ë²¡í„° ê²€ìƒ‰ì€ vector_search.pyì—ì„œ importí•´ì„œ ì‚¬ìš©í•˜ì„¸ìš”)")
    else:
        print("âŒ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤.")


# âœ… ì‹¤í–‰ ì§„ì…ì 
if __name__ == "__main__":
    if len(sys.argv) == 1:
        interactive_cli()
    else:
        parser = argparse.ArgumentParser()
        parser.add_argument("command", choices=["upload", "check", "preview", "delete"], help="ì‹¤í–‰ ëª…ë ¹")
        parser.add_argument("--field", help="ê²€ìƒ‰í•  í•„ë“œ (id, title, content)")
        parser.add_argument("--value", help="ê²€ìƒ‰ í‚¤ì›Œë“œ")
        parser.add_argument("--size", type=int, default=5, help="ë¯¸ë¦¬ë³´ê¸° ê°œìˆ˜ (ê¸°ë³¸: 5)")
        args = parser.parse_args()

        if args.command == "upload":
            print("ğŸš€ ì—…ë¡œë“œ ì‹œì‘")
            create_index_if_not_exists(index_name)
            convert_json_to_bulk()
            upload_bulk_jsonl()
        elif args.command == "check":
            check_index()
        elif args.command == "preview":
            preview_documents(size=args.size, field=args.field, value=args.value)
        elif args.command == "delete":
            delete_documents(field=args.field, value=args.value)
